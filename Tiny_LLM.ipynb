{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JustinJiangNext/Tiny-LLM/blob/main/Tiny_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIVXF0DBrhfA",
        "outputId": "fe0670cb-5415-4042-8903-b56629aa8d3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m132.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m98.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install torch tqdm requests\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title cuda\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLoPmQaqV_F4",
        "outputId": "c2353959-fd20-48de-cf94-7d3cb95447b7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "66ZAbFlhrl4x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92821b87-f01c-4b21-d1b7-d1094858f88b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading dataset poe_telltale\n",
            "number of chars in data 500000\n",
            "Preview:\n",
            "﻿The Project Gutenberg eBook of The Works of Edgar Allan Poe — Volume 2\r\n",
            "    \r\n",
            "This ebook is for the use of anyone anywhere in the United States and\r\n",
            "most other parts of the world at no cost and with almost no restrictions\r\n",
            "whatsoever. You may copy it, give it away or re-use it under the terms\r\n",
            "of the Project Gutenberg License included with this ebook or online\r\n",
            "at www.gutenberg.org. If you are no\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import requests\n",
        "\n",
        "datasets = {\n",
        "    \"shakespeare\": \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\",\n",
        "    \"poe_telltale\": \"https://www.gutenberg.org/cache/epub/2148/pg2148.txt\",#gluten berg\n",
        "    \"sherlock_holmes\": \"https://www.gutenberg.org/cache/epub/1661/pg1661.txt\",\n",
        "}\n",
        "\n",
        "selected_dataset = \"poe_telltale\"\n",
        "\n",
        "if selected_dataset in datasets:\n",
        "    url = datasets[selected_dataset]\n",
        "    response = requests.get(url)\n",
        "    data = response.text\n",
        "    print(f\"downloading dataset {selected_dataset}\")\n",
        "else:\n",
        "    raise ValueError(\"dataset doesnt exist\")\n",
        "\n",
        "#Too much data\n",
        "max_chars = 500_000\n",
        "if len(data) > max_chars:\n",
        "    data = data[:max_chars]\n",
        "\n",
        "# Save to file\n",
        "with open(\"training_text.txt\", \"w\", encoding='utf-8') as f:\n",
        "    f.write(data)\n",
        "\n",
        "print(f\"number of chars in data {len(data)}\")\n",
        "print(\"Preview:\\n\" + data[:400])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import tarfile\n",
        "import json\n",
        "import glob\n",
        "\n",
        "LITTLE_STORIES = True\n",
        "if LITTLE_STORIES:\n",
        "  DATA_DIR = \"data\"\n",
        "  ARCHIVE_NAME = \"TinyStories_all_data.tar.gz\"\n",
        "  ARCHIVE_URL = \"https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStories_all_data.tar.gz\"\n",
        "  EXTRACTED_DIR = os.path.join(DATA_DIR, \"TinyStories_all_data\")\n",
        "  OUTPUT_FILE = \"training_text.txt\"\n",
        "\n",
        "  os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "  archive_path = os.path.join(DATA_DIR, ARCHIVE_NAME)\n",
        "  if not os.path.exists(archive_path):\n",
        "      print(f\"downloading {ARCHIVE_URL}...\")\n",
        "      response = requests.get(ARCHIVE_URL, stream=True)\n",
        "      response.raise_for_status()\n",
        "      with open(archive_path, \"wb\") as f:\n",
        "          for chunk in response.iter_content(chunk_size=8192):\n",
        "              f.write(chunk)\n",
        "      print(\"finished downloading stories\")\n",
        "  else:\n",
        "      print(\"already downloaded the stories\")\n",
        "\n",
        "  if not os.path.exists(EXTRACTED_DIR):\n",
        "      print(\"unzipping archive of tories\")\n",
        "      with tarfile.open(archive_path, \"r:gz\") as tar:\n",
        "          tar.extractall(path=EXTRACTED_DIR)\n",
        "      print(\"finish unzipping\")\n",
        "  else:\n",
        "      print(\"already downloaded and unzipped stories\")\n",
        "\n",
        "  print(\"Going through JSON file to find the stories\")\n",
        "  MAX_STORIES = 1000\n",
        "  story_texts = []\n",
        "  json_files = glob.glob(os.path.join(EXTRACTED_DIR, \"*.json\"))\n",
        "  for json_file in json_files:\n",
        "      with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
        "          stories = json.load(f)\n",
        "          for story in stories:\n",
        "              if MAX_STORIES < 0:\n",
        "                  break\n",
        "              MAX_STORIES -= 1\n",
        "              text = story.get(\"story\", \"\").strip()\n",
        "              if text:\n",
        "                  story_texts.append(text)\n",
        "\n",
        "  data = \"\\n\\n\".join(story_texts)\n",
        "\n",
        "\n",
        "  with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "      f.write(data)\n",
        "\n",
        "  print(f\"number of chars in data {len(data)}\")\n",
        "  print(\"Preview:\\n\" + data[:400])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhWlEx0QvWau",
        "outputId": "8657f70e-a2ca-430a-b8f5-9e06769b9278"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "already downloaded the stories\n",
            "already downloaded and unzipped stories\n",
            "Going through JSON file to find the stories\n",
            "number of chars in data 766557\n",
            "Preview:\n",
            "Once upon a time, there was a little girl named May. She was three years old, and she loved to explore. One day, May asked her mom if she could go explore the subway. Her mom said yes, and May was so excited!\n",
            "May put on her coat and shoes, and off she went to the subway. She couldn't believe all the incredible things she saw! There were so many people, and the noise was incredible.\n",
            "As May was look\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ZiDGipWtrnS8"
      },
      "outputs": [],
      "source": [
        "# vocabs\n",
        "chars = sorted(list(set(data)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for ch, i in stoi.items()}\n",
        "\n",
        "#basic encode & decode\n",
        "def encode(s): return [stoi[c] for c in s]\n",
        "def decode(l): return ''.join([itos[i] for i in l])\n",
        "\n",
        "# encode dataset\n",
        "data_ids = encode(data)\n",
        "\n",
        "#splitting dataset\n",
        "import torch\n",
        "\n",
        "data_tensor = torch.tensor(data_ids, dtype=torch.long)\n",
        "n = int(0.9 * len(data_tensor))\n",
        "train_data = data_tensor[:n].to(device)\n",
        "val_data = data_tensor[n:].to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "cJCa_7GZrotr"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "block_size = 64\n",
        "batch_size = 32\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,), device=device)\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "exjYU5ozrqql"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "#stupid\n",
        "class TinyCharModel(nn.Module):\n",
        "    def __init__(self, vocab_size, n_embed):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, n_embed)\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "        embeds = self.token_embedding(x)\n",
        "        logits = self.lm_head(embeds)\n",
        "\n",
        "        if targets is None:\n",
        "            return logits, None\n",
        "        B, T, C = logits.shape\n",
        "        logits = logits.view(B*T, C)\n",
        "        targets = targets.view(B*T)\n",
        "        loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# === Hyperparameters ===\n",
        "vocab_size = len(chars)      # number of unique tokens (from tokenizer)\n",
        "n_embed = 128                # embedding size\n",
        "block_size = 256              # context length\n",
        "n_heads = 24                  # number of attention heads\n",
        "n_layers = 4                 # number of transformer blocks\n",
        "dropout = 0.1                # dropout rate\n",
        "\n",
        "class TinyTransformer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, n_embed)\n",
        "        self.pos_embedding = nn.Embedding(block_size, n_embed)\n",
        "        self.blocks = nn.Sequential(*[TransformerBlock(n_embed, n_heads) for _ in range(n_layers)])\n",
        "        self.ln_f = nn.LayerNorm(n_embed)\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding(idx)\n",
        "        pos_emb = self.pos_embedding(torch.arange(T, device=idx.device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            return logits, None\n",
        "        loss = F.cross_entropy(logits.view(-1, vocab_size), targets.view(-1))\n",
        "        return logits, loss\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, n_embed, n_heads):\n",
        "        super().__init__()\n",
        "        head_size = n_embed // n_heads\n",
        "        self.sa = MultiHeadAttention(n_heads, head_size)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(n_embed, 4 * n_embed),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embed, n_embed),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        self.ln1 = nn.LayerNorm(n_embed)\n",
        "        self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ff(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([SelfAttentionHead(head_size) for _ in range(n_heads)])\n",
        "        self.proj = nn.Linear(n_heads * head_size, n_embed)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        return self.dropout(self.proj(out))\n",
        "\n",
        "class SelfAttentionHead(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        wei = q @ k.transpose(-2, -1) / (C ** 0.5)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x)\n",
        "        return wei @ v\n"
      ],
      "metadata": {
        "id": "n9gdbQxls7IQ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "8_tw52uIrsdd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18a18bff-690a-490c-8ce9-76da550b2604"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: train loss 4.4804\n",
            "Step 100: train loss 2.3380\n",
            "Step 200: train loss 2.2883\n",
            "Step 300: train loss 2.2475\n",
            "Step 400: train loss 2.1702\n",
            "Step 500: train loss 2.0281\n",
            "Step 600: train loss 1.8686\n",
            "Step 700: train loss 1.7504\n",
            "Step 800: train loss 1.7565\n",
            "Step 900: train loss 1.5542\n",
            "Step 1000: train loss 1.4942\n",
            "Step 1100: train loss 1.4837\n",
            "Step 1200: train loss 1.4519\n",
            "Step 1300: train loss 1.3749\n",
            "Step 1400: train loss 1.2928\n"
          ]
        }
      ],
      "source": [
        "#model = TinyCharModel()\n",
        "model = TinyTransformer().to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "for step in range(1500):\n",
        "    model.train()\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % 100 == 0:\n",
        "        print(f\"Step {step}: train loss {loss.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "JIw7RWYjruaN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d347c5ab-8bf4-4a87-d485-4312d2432bfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tetris is the best game in the store and the little bear for the dog. They took the the park with the boy and saw a big his friends her friends. They was sad so happy and said, \"What to the can the store and the park. It was be to \n"
          ]
        }
      ],
      "source": [
        "def generate(model, start_str, max_new_tokens=200):\n",
        "    model.eval()\n",
        "    idx = torch.tensor(encode(start_str), dtype=torch.long)[None, :].to(device)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        logits, _ = model(idx[:, -block_size:])\n",
        "        logits = logits[:, -1, :]\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        next_token = torch.multinomial(probs, num_samples=1)\n",
        "        idx = torch.cat((idx, next_token), dim=1)\n",
        "\n",
        "    return decode(idx[0].tolist())\n",
        "\n",
        "def generate(model, start_str, max_new_tokens = 200, temperature=0.3, top_k=20):\n",
        "    model.eval()\n",
        "    idx = torch.tensor(encode(start_str), dtype=torch.long)[None, :].to(device)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -block_size:]\n",
        "        logits, _ = model(idx_cond)\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "        if top_k is not None:\n",
        "            v, _ = torch.topk(logits, top_k)\n",
        "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        next_token = torch.multinomial(probs, num_samples=1)\n",
        "        idx = torch.cat((idx, next_token), dim=1)\n",
        "    return decode(idx[0].tolist())\n",
        "\n",
        "print(generate(model, \"Tetris is the best game in the \"))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOU6rmoAQzaQYTv2n6Pasxu",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}